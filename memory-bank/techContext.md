# Technical Context

## Technology Stack

### Backend
- **Language**: Python 3.9+
- **Framework**: Flask (lightweight REST API)
- **LLM SDK**: Azure OpenAI Python SDK (openai==1.55.3)
- **Dependencies**:
  - `openai` - Azure OpenAI client
  - `flask` - Web framework
  - `flask-cors` - Handle CORS for React frontend
  - `python-dotenv` - Environment variable management

### Frontend
- **Framework**: React 18+
- **Build Tool**: Vite
- **Styling**: Modern CSS with gradients and animations
- **HTTP Client**: Fetch API
- **Rendering**: Markdown support with `dangerouslySetInnerHTML`

## Project Structure
```
onboarding-chatbot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py              # Flask application + format tool logic
â”‚   â”œâ”€â”€ mock_data.py        # Mock data (employees, teams, tasks, KB)
â”‚   â”œâ”€â”€ functions.py        # Function calling + FORMAT_RESPONSE_TOOL
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ .env.example        # Environment variables template
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx         # Main component + suggestion chips
â”‚   â”‚   â”œâ”€â”€ App.css         # Styles including .suggestion-chip
â”‚   â”‚   â””â”€â”€ main.jsx        # Entry point
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ memory-bank/            # Project documentation
â”‚   â”œâ”€â”€ activeContext.md
â”‚   â”œâ”€â”€ progress.md
â”‚   â”œâ”€â”€ systemPatterns.md
â”‚   â”œâ”€â”€ techContext.md
â”‚   â”œâ”€â”€ productContext.md
â”‚   â””â”€â”€ projectbrief.md
â”œâ”€â”€ README.md               # Main documentation
â”œâ”€â”€ IMPROVEMENTS.md         # Phase 1 changelog
â”œâ”€â”€ USE_CASES.md           # Phase 2 use cases
â””â”€â”€ CONTEXTUAL_SUGGESTIONS.md  # Phase 3 suggestions
```

## Development Setup
1. Backend runs on `http://localhost:5000`
2. Frontend runs on `http://localhost:5173` (Vite default)
3. CORS enabled for local development

## Azure OpenAI Configuration
Required environment variables:
- `AZURE_OPENAI_ENDPOINT` - Your Azure OpenAI endpoint URL
- `AZURE_OPENAI_API_KEY` - API key for authentication
- `AZURE_OPENAI_DEPLOYMENT_NAME` - Deployment name (e.g., gpt-4)
- `AZURE_OPENAI_API_VERSION` - API version (2024-02-15-preview)

## API Endpoints

### POST /api/chat
Main chat endpoint with contextual suggestions.

**Request:**
```json
{
  "messages": [
    {"role": "user", "content": "Nhiá»‡m vá»¥ cá»§a tÃ´i lÃ  gÃ¬?"}
  ]
}
```

**Response:**
```json
{
  "success": true,
  "messages": [...],
  "response": {
    "role": "assistant",
    "content": "ÄÃ¢y lÃ  cÃ¡c nhiá»‡m vá»¥ cá»§a anh...",
    "suggested_prompts": [
      "ÄÃ¡nh dáº¥u task hoÃ n thÃ nh",
      "Task nÃ o sáº¯p háº¿t háº¡n?",
      "Ai lÃ  buddy cá»§a tÃ´i?"
    ]
  }
}
```

### POST /api/greeting
Generate personalized greeting with deadline alerts.

**Request:**
```json
{
  "employee_id": "E123"
}
```

**Response:**
```json
{
  "success": true,
  "greeting": "ðŸ‘‹ ChÃ o An! Em lÃ  Trá»£ lÃ½ Onboarding...",
  "urgent_tasks_count": 2
}
```

### GET /api/health
Health check endpoint.

## Key Implementation Details

### Format Tool Integration
- Defined in `backend/functions.py` as `FORMAT_RESPONSE_TOOL`
- Included in `ALL_TOOLS` array
- Forced via `function_call={"name": "format_user_response"}`
- Parsed in `backend/app.py` to extract structured output

### Contextual Suggestions
- Generated by LLM based on conversation context
- Always 3 suggestions per response
- Displayed as clickable chips in UI
- Click handler: `onClick={() => setInputValue(prompt)}`

### Performance Considerations
- **Latency**: +500-800ms per response (2 LLM calls)
- **Token Usage**: +80-150 tokens per interaction
- **Trade-off**: Better UX worth the cost

## Dependencies

### Backend (requirements.txt)
```
Flask==3.0.0
flask-cors==4.0.0
openai==1.55.3
python-dotenv==1.0.0
```

### Frontend (package.json)
```json
{
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0"
  },
  "devDependencies": {
    "@vitejs/plugin-react": "^4.2.1",
    "vite": "^5.0.8"
  }
}
```
